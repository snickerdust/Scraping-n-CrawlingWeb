{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRAKTIKUM ALGORITMA PEMROGRAMAN 2 ##\n",
    "### TM - 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nama : Rafli Maulana Firmansyah \n",
    "\n",
    "\n",
    "NIM : 162112133064"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tugas Praktikum \n",
    "\n",
    "\n",
    "1.\tDengan menggunakan Python Requests dan BeautifulSoup cobalah untuk scraping 1 halaman website unair news https://unair.ac.id/news\n",
    "2.\tLalu, lakukan crawling unair news dengan mengkombinasikan Python Request, beautifulsoup, dan for loop\n",
    "3.\tSetelah itu, gunakan scrapy untuk melakukan crawling website https://store.playstation.com/en-id \n",
    "4.\tSimpan file hasil crawling dalam Repo github masing masing. (Bisa dalam Json / csv)\n",
    "5.\tTulis laporan menggunakan ipynb dan render/save as menjadi pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "import scrapy as sc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping dengan Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah - Langkah : \n",
    "1. Melakukan html request dan menyimpannya kedalam suatu variabel\n",
    "2. Memasukkan kedalam Beautifulsoup\n",
    "3. Find_all tag dan class yang akan dicari\n",
    "4. Bersihkan data tersebut dari residu yang tidak diperlukan \n",
    "5. Masukkan kedalam dataframe dan disimpan kedalam csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Request HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = req.get(\"https://unair.ac.id/news\")\n",
    "outputrespon = url.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memasukkan kedalam BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(outputrespon, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya mengkonversi data html kedalam BeautifulSoup agar dapat melakukan scrapping didalamnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find_all tag yang akan dicari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_judul = soup.find_all('div', class_='elementor-post__excerpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya mengambil Find_all dari tag div dan class \"elementor-post__excerpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mengambil bagian yang diperlukan dari tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divp = []\n",
    "\n",
    "for i in list_judul:\n",
    "    findp = i.find('p').get_text()\n",
    "    divp.append(findp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya mengambil isi tag p dari div diatas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memasukkan kedalam dataframe lalu dibuat dalam bentuk csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judul_Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNAIR NEWS – Universitas Airlangga membuka pel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dalam rangka memperingati Hari Cuci Tangan Pak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dalam rangka memperingati Hari Cuci Tangan Pak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dalam rangka memperingati Hari Cuci Tangan Pak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNAIR NEWS – Sekolah Pascasarjana Universitas ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Judul_Headline\n",
       "0  UNAIR NEWS – Universitas Airlangga membuka pel...\n",
       "1  Dalam rangka memperingati Hari Cuci Tangan Pak...\n",
       "2  Dalam rangka memperingati Hari Cuci Tangan Pak...\n",
       "3  Dalam rangka memperingati Hari Cuci Tangan Pak...\n",
       "4  UNAIR NEWS – Sekolah Pascasarjana Universitas ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(divp)\n",
    "df.columns = ['Judul_Headline']\n",
    "df.to_csv('scraping-divp.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari output diatas dapat dilihat bahwa data sudah terbentuk dalam dataset dan berisi sesuai dengan yang ada di website unair news tersebut dan dibuat dalam bentuk output csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah - Langkah : \n",
    "1. Melakukan html request dan menyimpannya kedalam suatu variabel\n",
    "2. Memasukkan kedalam Beautifulsoup\n",
    "3. Find_all tag dan class yang akan dicari\n",
    "4. Bersihkan data tersebut dari residu yang tidak diperlukan \n",
    "5. Masukkan kedalam dataframe dan disimpan kedalam csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = req.get('https://unair.ac.id/news')\n",
    "outputurl1 = url1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya melakukan http request dan dismpan dalam variabel outputurl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = bs(outputurl1, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu pada bagian ini saya melakukan konversi data http request kedalam BeatifulSoup untuk diproses nantinya "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Membuat Rute Crawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup1.find_all('h3', class_='elementor-post__title')\n",
    "\n",
    "abaru = []\n",
    "for i in all_links:\n",
    "    abc = i.find('a')\n",
    "    abaru.append(abc)\n",
    "\n",
    "hrefbaru = []\n",
    "for i in abaru: \n",
    "    hr = i['href']\n",
    "    hrefbaru.append(hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya membuat rute crawler dengan melakukan find_all pada tag h3 class \"elementor-post__title\" lalu melakukan iterasi berbentuk list untuk memfilter href atau link yan ada pada tag tersebut dan dimasukkan kedalam list baru bernma hrefbaru = [] agar dapat diproses kedalam dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Melakukan Crawl dari hrefbaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judulweb = []\n",
    "waktu = []\n",
    "#crawler unit (masih bisa dikembangkan)\n",
    "for i in hrefbaru:\n",
    "    hapus = req.get(i.lstrip(\"\\'\"))\n",
    "    hilang = bs(hapus.text, 'html.parser')\n",
    "    judul = hilang.find_all('h3', class_='elementor-heading-title elementor-size-default')\n",
    "    waktuweb = hilang.find_all('span', class_='elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-time')\n",
    "\n",
    "    judulweb.append(judul[0].string)\n",
    "    waktu.append(waktuweb[0].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada baian ini saya melakukan crawl dengan data link dari hrefbaru, lalu tiap link mengambil tag h3 dengan class \"elementor-heading-title elementor-size-default\" dan span dengan class \"elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-time\". Mereka yaitu judul dari link tersebut dan waktu upload artikel tersebut yang dimasukkan kedalam list judulweb untuk judul dan waktu untuk waktu link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Membersihkan residu data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_waktu = []\n",
    "filtered_waktu = []\n",
    "\n",
    "for a in waktu: \n",
    "    splitted = re.split(r'[\\n\\t\\f\\v\\r]+', a)\n",
    "    for i in splitted[1:]:\n",
    "        raw_waktu.append(i)\n",
    "for x in raw_waktu[0::2]:\n",
    "    filtered_waktu.append(x)\n",
    "filtered_waktu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini saya melakukan pembersihan terhadap data link dari waktu karena masih terdapat banyak residu seperti /n dan /t dari html tersebut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judul</th>\n",
       "      <th>Waktu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNAIR Terima Penambahan Bus Operasional Kampus...</td>\n",
       "      <td>11:59 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tingkatkan Motorik Anak dengan Kegiatan Lomba ...</td>\n",
       "      <td>12:33 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNAIR Terima Penambahan Bus Operasional Kampus...</td>\n",
       "      <td>11:59 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aturan Baru Kemenag Tentang Pelecehan Seksual ...</td>\n",
       "      <td>10:34 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aksi Sosial Donor Darah FISIP UNAIR dalam Samb...</td>\n",
       "      <td>8:47 am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Judul     Waktu\n",
       "0  UNAIR Terima Penambahan Bus Operasional Kampus...  11:59 am\n",
       "1  Tingkatkan Motorik Anak dengan Kegiatan Lomba ...  12:33 pm\n",
       "2  UNAIR Terima Penambahan Bus Operasional Kampus...  11:59 am\n",
       "3  Aturan Baru Kemenag Tentang Pelecehan Seksual ...  10:34 am\n",
       "4  Aksi Sosial Donor Darah FISIP UNAIR dalam Samb...   8:47 am"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Judul' : judulweb, 'Waktu' : filtered_waktu})\n",
    "df2.to_csv('crawlingjudulwaktu.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari output diatas dapat dilihat bahwa data sudah terbentuk dalam dataset dan berisi sesuai dengan yang ada di link website unair news tersebut dan dibuat dalam bentuk output csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy Crawler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah - Langkah : \n",
    "1. Set up Virtual Environment \n",
    "2. Set up Scrapy Environment\n",
    "3. Menjalankan Spider Code dengan scrapy \n",
    "4. Output data yang diinginkan dalam bentuk .json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Up Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spider code\n",
    "import scrapy\n",
    "\n",
    "class melata(scrapy.Spider):\n",
    "    name = 'melatakuys' #nama spider\n",
    "    start_urls = [\"https://store.playstation.com/en-id/search/f1\"] #url awal\n",
    "    \n",
    "\n",
    "    def parse(self, response):\n",
    "        url = response.url\n",
    "        \n",
    "        for i in range(1,6): #iterasi sampe page ke-46\n",
    "            yield scrapy.Request(url=url+str(i), callback=self.parse_details) #url awal ditambah page ke-sekian\n",
    "            \n",
    "    def parse_details(self, response):\n",
    "        for text in response.css(\".psw-product-tile\"): \n",
    "            yield{\n",
    "                \"title\":text.css(\".psw-t-body::text\").get(), #filter yang memiliki class .psw-t-body\n",
    "                \"price\":text.css(\".psw-m-r-3::text\").get()}  #filter yang memiliki class .psw-m-r-3\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38593fcefe49566d83855c8d11215a5842de27570f36030aa311eb722faa2984"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
